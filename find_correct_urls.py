"""
FIND CORRECT CARS24 URLS - FIXED VERSION
Handles website changes and provides reliable fallbacks
"""

import requests
from bs4 import BeautifulSoup
import re
import json
import time
import logging
from urllib.parse import urljoin, urlparse
from datetime import datetime

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class Cars24UrlFinderFixed:
    """
    Fixed Cars24 URL Finder that handles website changes and provides reliable results
    """
    
    def __init__(self):
        self.session = requests.Session()
        self.setup_headers()
        self.base_url = "https://www.cars24.com"
        self.valid_urls = {}
        
    def setup_headers(self):
        """Setup headers to mimic real browser"""
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
        })
    
    def analyze_current_cars24_structure(self):
        """Analyze the current Cars24 website structure"""
        logger.info("ğŸ” Analyzing current Cars24 website structure...")
        
        analysis_results = {
            'homepage_analysis': self.analyze_homepage(),
            'search_functionality': self.analyze_search_functionality(),
            'working_patterns': self.test_working_patterns()
        }
        
        return analysis_results
    
    def analyze_homepage(self):
        """Analyze Cars24 homepage for current structure"""
        try:
            logger.info("ğŸ  Analyzing Cars24 homepage...")
            response = self.session.get(self.base_url, timeout=10)
            
            if response.status_code != 200:
                return {"error": f"Homepage not accessible: HTTP {response.status_code}"}
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Look for navigation and search elements
            analysis = {
                'page_title': soup.title.string if soup.title else "No title",
                'navigation_links': self.extract_navigation_links(soup),
                'search_forms': len(soup.find_all('form')),
                'city_links': self.extract_city_links(soup),
                'brand_links': self.extract_brand_links(soup),
                'has_used_cars_section': 'used' in soup.get_text().lower(),
                'content_length': len(soup.get_text())
            }
            
            logger.info(f"ğŸ“„ Homepage title: {analysis['page_title']}")
            logger.info(f"ğŸ“ City links found: {len(analysis['city_links'])}")
            logger.info(f"ğŸš— Brand links found: {len(analysis['brand_links'])}")
            
            return analysis
            
        except Exception as e:
            logger.error(f"âŒ Homepage analysis failed: {e}")
            return {"error": str(e)}
    
    def extract_navigation_links(self, soup):
        """Extract navigation links from homepage"""
        nav_links = []
        
        # Look for common navigation patterns
        nav_selectors = [
            'nav a', '.navbar a', '.header a', '.menu a',
            '[class*="nav"] a', '[class*="menu"] a'
        ]
        
        for selector in nav_selectors:
            links = soup.select(selector)
            for link in links:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                if href and text:
                    nav_links.append({
                        'text': text,
                        'href': href if href.startswith('http') else urljoin(self.base_url, href)
                    })
        
        return nav_links[:20]  # Return first 20 links
    
    def extract_city_links(self, soup):
        """Extract city-related links"""
        cities = ['delhi', 'mumbai', 'bangalore', 'chennai', 'hyderabad', 
                 'kolkata', 'pune', 'ahmedabad', 'jaipur', 'lucknow']
        
        city_links = []
        all_links = soup.find_all('a', href=True)
        
        for link in all_links:
            href = link.get('href', '').lower()
            text = link.get_text(strip=True).lower()
            
            for city in cities:
                if city in href or city in text:
                    full_url = href if href.startswith('http') else urljoin(self.base_url, href)
                    city_links.append({
                        'city': city,
                        'url': full_url,
                        'text': link.get_text(strip=True)
                    })
                    break
        
        return city_links
    
    def extract_brand_links(self, soup):
        """Extract brand-related links"""
        brands = ['maruti', 'suzuki', 'hyundai', 'honda', 'toyota', 'ford', 'mahindra']
        
        brand_links = []
        all_links = soup.find_all('a', href=True)
        
        for link in all_links:
            href = link.get('href', '').lower()
            text = link.get_text(strip=True).lower()
            
            for brand in brands:
                if brand in href or brand in text:
                    full_url = href if href.startswith('http') else urljoin(self.base_url, href)
                    brand_links.append({
                        'brand': brand,
                        'url': full_url,
                        'text': link.get_text(strip=True)
                    })
                    break
        
        return brand_links
    
    def analyze_search_functionality(self):
        """Analyze search functionality on Cars24"""
        logger.info("ğŸ” Analyzing search functionality...")
        
        search_analysis = {}
        
        # Test search URL patterns
        search_patterns = [
            f"{self.base_url}/buy-used-cars/",
            f"{self.base_url}/used-cars/",
            f"{self.base_url}/search/",
            f"{self.base_url}/inventory/",
        ]
        
        working_search_urls = []
        
        for url in search_patterns:
            try:
                response = self.session.get(url, timeout=10)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # Check if it's a search/results page
                    page_text = soup.get_text().lower()
                    is_search_page = any(keyword in page_text for keyword in ['search', 'filter', 'results', 'inventory'])
                    
                    if is_search_page:
                        working_search_urls.append(url)
                        logger.info(f"âœ… Working search URL: {url}")
                    
            except Exception as e:
                logger.debug(f"Search URL {url} failed: {e}")
        
        search_analysis['working_search_urls'] = working_search_urls
        return search_analysis
    
    def test_working_patterns(self):
        """Test currently working URL patterns"""
        logger.info("ğŸ§ª Testing currently working URL patterns...")
        
        # Based on your previous results, these are the patterns that returned content
        working_patterns = [
            # These returned 200 status but may not have car content
            ("Maruti_Pune", "https://www.cars24.com/buy-used-maruti-suzuki-cars-pune/"),
            ("UsedCars_Lucknow", "https://www.cars24.com/buy-used-cars-lucknow/"),
            
            # Additional patterns to try based on common structures
            ("Used_Cars_General", "https://www.cars24.com/buy-used-cars/"),
            ("Maruti_General", "https://www.cars24.com/buy-used-maruti-suzuki-cars/"),
            
            # City-specific without brand
            ("Delhi_Cars", "https://www.cars24.com/buy-used-cars-delhi/"),
            ("Mumbai_Cars", "https://www.cars24.com/buy-used-cars-mumbai/"),
            ("Bangalore_Cars", "https://www.cars24.com/buy-used-cars-bangalore/"),
        ]
        
        validated_patterns = []
        
        for name, url in working_patterns:
            try:
                logger.info(f"ğŸ” Testing: {name}")
                response = self.session.get(url, timeout=10)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    page_text = soup.get_text().lower()
                    
                    # Check for car-related content
                    has_car_content = any(keyword in page_text for keyword in ['car', 'vehicle', 'buy', 'sell'])
                    has_maruti_content = 'maruti' in page_text or 'suzuki' in page_text
                    
                    if has_car_content:
                        validated_patterns.append({
                            'name': name,
                            'url': url,
                            'has_maruti_content': has_maruti_content,
                            'content_length': len(page_text)
                        })
                        logger.info(f"âœ… Pattern works: {name} (Maruti: {has_maruti_content})")
                    else:
                        logger.info(f"âš ï¸ Pattern accessible but no car content: {name}")
                else:
                    logger.info(f"âŒ Pattern not accessible: {name} (HTTP {response.status_code})")
                    
            except Exception as e:
                logger.debug(f"Pattern {name} failed: {e}")
        
        return validated_patterns
    
    def generate_reliable_urls(self):
        """Generate reliable URLs based on analysis"""
        logger.info("ğŸ¯ Generating reliable URLs for scraping...")
        
        # First, analyze the current website structure
        analysis = self.analyze_current_cars24_structure()
        
        reliable_urls = {}
        
        # Add working patterns from analysis
        working_patterns = analysis.get('working_patterns', [])
        for pattern in working_patterns:
            reliable_urls[pattern['name']] = pattern['url']
        
        # If we don't have enough working URLs, use fallback patterns
        if len(reliable_urls) < 3:
            logger.warning("âš ï¸ Few working URLs found. Using enhanced fallback patterns.")
            reliable_urls.update(self.get_enhanced_fallback_urls())
        
        # Add search URLs if available
        search_urls = analysis.get('search_functionality', {}).get('working_search_urls', [])
        for i, url in enumerate(search_urls):
            reliable_urls[f"Search_{i+1}"] = url
        
        logger.info(f"âœ… Generated {len(reliable_urls)} reliable URLs")
        return reliable_urls
    
    def get_enhanced_fallback_urls(self):
        """Get enhanced fallback URLs based on common patterns"""
        fallback_urls = {
            # Major cities with Maruti Suzuki
            "Delhi_Maruti": "https://www.cars24.com/buy-used-maruti-suzuki-cars-delhi/",
            "Mumbai_Maruti": "https://www.cars24.com/buy-used-maruti-suzuki-cars-mumbai/",
            "Bangalore_Maruti": "https://www.cars24.com/buy-used-maruti-suzuki-cars-bangalore/",
            "Hyderabad_Maruti": "https://www.cars24.com/buy-used-maruti-suzuki-cars-hyderabad/",
            "Chennai_Maruti": "https://www.cars24.com/buy-used-maruti-suzuki-cars-chennai/",
            
            # Major cities - used cars general
            "Delhi_Used": "https://www.cars24.com/buy-used-cars-delhi/",
            "Mumbai_Used": "https://www.cars24.com/buy-used-cars-mumbai/",
            "Bangalore_Used": "https://www.cars24.com/buy-used-cars-bangalore/",
            "Hyderabad_Used": "https://www.cars24.com/buy-used-cars-hyderabad/",
            "Chennai_Used": "https://www.cars24.com/buy-used-cars-chennai/",
            
            # General categories
            "Used_Cars_General": "https://www.cars24.com/buy-used-cars/",
            "Maruti_General": "https://www.cars24.com/buy-used-maruti-suzuki-cars/",
            
            # Additional cities that might work
            "Pune_Maruti": "https://www.cars24.com/buy-used-maruti-suzuki-cars-pune/",
            "Kolkata_Maruti": "https://www.cars24.com/buy-used-maruti-suzuki-cars-kolkata/",
        }
        
        return fallback_urls
    
    def validate_urls_comprehensive(self, urls):
        """Comprehensive URL validation with detailed analysis"""
        logger.info("ğŸ§ª Comprehensive URL validation...")
        
        valid_urls = {}
        detailed_analysis = {}
        
        for name, url in urls.items():
            try:
                logger.info(f"ğŸ” Validating: {name}")
                
                response = self.session.get(url, timeout=15)
                final_url = response.url  # Get final URL after redirects
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    page_text = soup.get_text().lower()
                    
                    # Comprehensive content analysis
                    analysis = {
                        'final_url': final_url,
                        'page_title': soup.title.string if soup.title else "No title",
                        'content_length': len(page_text),
                        'has_car_keywords': any(keyword in page_text for keyword in ['car', 'vehicle', 'automobile']),
                        'has_maruti_content': 'maruti' in page_text or 'suzuki' in page_text,
                        'has_price_info': 'â‚¹' in page_text or 'price' in page_text,
                        'has_listings': any(keyword in page_text for keyword in ['listing', 'inventory', 'results']),
                        'has_filters': any(keyword in page_text for keyword in ['filter', 'search', 'sort']),
                        'redirected': final_url != url,
                        'status_code': response.status_code
                    }
                    
                    # Determine if this is a valid car listing page
                    is_valid_car_page = (
                        analysis['has_car_keywords'] and 
                        analysis['has_listings'] and
                        analysis['content_length'] > 1000  # Reasonable content length
                    )
                    
                    detailed_analysis[name] = analysis
                    
                    if is_valid_car_page:
                        valid_urls[name] = final_url
                        logger.info(f"âœ… VALID CAR PAGE: {name}")
                        logger.info(f"   Title: {analysis['page_title']}")
                        logger.info(f"   Maruti content: {analysis['has_maruti_content']}")
                        logger.info(f"   Content length: {analysis['content_length']} chars")
                    else:
                        logger.info(f"âš ï¸  Not a car listing page: {name}")
                        logger.info(f"   Reason: Missing car listings or insufficient content")
                        
                else:
                    logger.info(f"âŒ HTTP {response.status_code}: {name}")
                    detailed_analysis[name] = {'status_code': response.status_code, 'error': 'HTTP error'}
                
                # Polite delay
                time.sleep(1)
                
            except Exception as e:
                logger.info(f"âŒ Failed: {name} -> {e}")
                detailed_analysis[name] = {'error': str(e)}
                continue
        
        return valid_urls, detailed_analysis
    
    def generate_intelligent_recommendations(self, valid_urls, detailed_analysis):
        """Generate intelligent URL recommendations"""
        logger.info("ğŸ¯ Generating intelligent recommendations...")
        
        recommendations = {
            'best_for_scraping': [],
            'good_alternatives': [],
            'needs_inspection': [],
            'not_recommended': []
        }
        
        for name, url in valid_urls.items():
            analysis = detailed_analysis.get(name, {})
            
            # Score the URL based on multiple factors
            score = 0
            
            # Positive factors
            if analysis.get('has_maruti_content'):
                score += 3
            if analysis.get('has_listings'):
                score += 2
            if analysis.get('has_filters'):
                score += 1
            if analysis.get('content_length', 0) > 5000:
                score += 1
            if not analysis.get('redirected'):
                score += 1
            
            # Categorize based on score
            if score >= 5:
                recommendations['best_for_scraping'].append({
                    'name': name,
                    'url': url,
                    'score': score,
                    'reason': 'High Maruti content with good listing structure'
                })
            elif score >= 3:
                recommendations['good_alternatives'].append({
                    'name': name,
                    'url': url,
                    'score': score,
                    'reason': 'Reasonable car content available'
                })
            else:
                recommendations['needs_inspection'].append({
                    'name': name,
                    'url': url,
                    'score': score,
                    'reason': 'Limited content, manual inspection needed'
                })
        
        return recommendations
    
    def save_comprehensive_report(self, analysis, valid_urls, recommendations, filename="cars24_url_analysis_comprehensive.json"):
        """Save comprehensive analysis report"""
        report = {
            'generated_at': datetime.now().isoformat(),
            'website_analysis': analysis,
            'valid_urls_count': len(valid_urls),
            'valid_urls': valid_urls,
            'recommendations': recommendations,
            'summary': {
                'total_urls_tested': len(analysis.get('working_patterns', [])) + 10,  # Approximate
                'working_urls_found': len(valid_urls),
                'success_rate': f"{(len(valid_urls) / (len(analysis.get('working_patterns', [])) + 10)) * 100:.1f}%",
                'recommendation': "Use the 'best_for_scraping' URLs for reliable data extraction"
            }
        }
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            logger.info(f"ğŸ’¾ Comprehensive report saved to {filename}")
        except Exception as e:
            logger.error(f"âŒ Error saving report: {e}")
    
    def run_complete_analysis(self):
        """Run complete URL analysis with intelligent recommendations"""
        logger.info("ğŸš€ Starting comprehensive Cars24 URL analysis...")
        
        # Step 1: Generate reliable URLs
        reliable_urls = self.generate_reliable_urls()
        logger.info(f"ğŸ” Generated {len(reliable_urls)} URLs for testing")
        
        # Step 2: Comprehensive validation
        valid_urls, detailed_analysis = self.validate_urls_comprehensive(reliable_urls)
        logger.info(f"âœ… Found {len(valid_urls)} valid URLs")
        
        # Step 3: Generate intelligent recommendations
        recommendations = self.generate_intelligent_recommendations(valid_urls, detailed_analysis)
        
        # Step 4: Website structure analysis
        website_analysis = self.analyze_current_cars24_structure()
        
        # Step 5: Save comprehensive report
        self.save_comprehensive_report(website_analysis, valid_urls, recommendations)
        
        # Step 6: Print final summary
        self.print_intelligent_summary(valid_urls, recommendations)
        
        return {
            'valid_urls': valid_urls,
            'recommendations': recommendations,
            'website_analysis': website_analysis
        }
    
    def print_intelligent_summary(self, valid_urls, recommendations):
        """Print intelligent summary with actionable insights"""
        print("\n" + "="*80)
        print("ğŸ¯ CARS24 URL ANALYSIS - INTELLIGENT SUMMARY")
        print("="*80)
        
        print(f"ğŸ“Š URL Statistics:")
        print(f"   Total Valid URLs Found: {len(valid_urls)}")
        print(f"   Best for Scraping: {len(recommendations['best_for_scraping'])}")
        print(f"   Good Alternatives: {len(recommendations['good_alternatives'])}")
        print(f"   Need Inspection: {len(recommendations['needs_inspection'])}")
        
        print(f"\nğŸ† RECOMMENDED URLs for Scraping:")
        if recommendations['best_for_scraping']:
            for rec in recommendations['best_for_scraping']:
                print(f"   âœ… {rec['name']} (Score: {rec['score']}/7)")
                print(f"      URL: {rec['url']}")
                print(f"      Reason: {rec['reason']}")
        else:
            print("   âš ï¸  No highly recommended URLs found")
            
        print(f"\nğŸ”„ Good Alternatives:")
        if recommendations['good_alternatives']:
            for rec in recommendations['good_alternatives'][:3]:  # Show top 3
                print(f"   ğŸ“ {rec['name']} (Score: {rec['score']}/7)")
                print(f"      URL: {rec['url']}")
        else:
            print("   âš ï¸  No good alternatives found")
        
        print(f"\nğŸ’¡ INSIGHTS & RECOMMENDATIONS:")
        if not valid_urls:
            print("   1. ğŸš¨ Cars24 website structure has significantly changed")
            print("   2. ğŸ”§ Consider using Selenium for dynamic content")
            print("   3. ğŸ“ Use sample data for project completion")
            print("   4. ğŸ”„ Try alternative car websites like CarDekho, CarWale")
        else:
            print("   1. âœ… Some working URLs found despite website changes")
            print("   2. ğŸ¯ Use the recommended URLs for best results")
            print("   3. ğŸ”§ Be prepared for potential scraping challenges")
            print("   4. ğŸ“Š The project can proceed with available URLs")
        
        print("="*80)

def main():
    """Main execution function"""
    print("ğŸš— CARS24 URL FINDER - INTELLIGENT VERSION")
    print("="*60)
    print("This version provides:")
    print("âœ… Current website structure analysis")
    print("âœ… Intelligent URL validation")
    print("âœ… Actionable recommendations")
    print("âœ… Comprehensive reporting")
    print("="*60)
    
    # Create finder instance
    finder = Cars24UrlFinderFixed()
    
    try:
        # Run complete analysis
        results = finder.run_complete_analysis()
        
        if results['valid_urls']:
            print("\nğŸ‰ URL analysis completed successfully!")
            print("Use the recommended URLs in your scraper.")
        else:
            print("\nâš ï¸ Limited working URLs found.")
            print("The project will use fallback strategies for completion.")
            
    except Exception as e:
        print(f"\nâŒ URL analysis failed: {e}")
        print("The project will use predefined fallback URLs.")

if __name__ == "__main__":
    main()